# ğŸ‘ï¸ Observability Cheat Sheet

> **Purpose:** Production-grade observability â€” metrics, logging, tracing, alerting, and SLI/SLO definitions. Reference before instrumenting any service, defining alerts, or investigating production issues.
> **Stack context:** Java 21+ / Spring Boot 3.x / Micrometer / Prometheus / Grafana / OpenTelemetry / ELK/Loki

---

## ğŸ“‹ The Three Pillars + Context

```
                    OBSERVABILITY
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ METRICS  â”‚    â”‚   LOGGING    â”‚   â”‚ TRACES  â”‚
   â”‚ (Numbers)â”‚    â”‚   (Events)   â”‚   â”‚ (Flows) â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                â”‚                â”‚
   What is the      What happened     Where did the
   system DOING?    at this MOMENT?   request GO?
        â”‚                â”‚                â”‚
   Prometheus/       ELK / Loki /     Jaeger / Tempo /
   Datadog/          CloudWatch       Zipkin / OTLP
   CloudWatch
        â”‚                â”‚                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ ALERTS  â”‚
                    â”‚ (Action)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  When should humans
                  be NOTIFIED?
```

### The Observability Decision: Which Pillar When?

| Question | Pillar | Tool |
|----------|--------|------|
| "Is the system healthy RIGHT NOW?" | **Metrics** | Dashboard gauges |
| "What's the p99 latency trend this week?" | **Metrics** | Time-series graph |
| "WHY did payment pay-123 fail?" | **Logs** | Log search by correlation ID |
| "WHERE did the request spend time?" | **Traces** | Distributed trace view |
| "Is the SLO being met this month?" | **Metrics** | SLO burn rate dashboard |
| "Should someone wake up at 3am?" | **Alerts** | Alert rule evaluation |

---

## ğŸ“Š Pattern 1: Metrics with Micrometer

### Metric Types â€” When to Use Each

```
COUNTER     â€” Things that only go UP
              "How many payments processed?"
              "How many errors occurred?"
              Use: rate() in Prometheus to see per-second throughput

GAUGE       â€” Current VALUE that goes up and down
              "How many connections are active?"
              "What's the current queue depth?"
              "How much heap memory is used?"

TIMER       â€” DURATION of things + count
              "How long do gateway calls take?"
              "What's the p99 request latency?"
              Combines: counter (of invocations) + distribution (of durations)

DISTRIBUTION â€” Distribution of VALUES (not time)
SUMMARY        "What's the payment amount distribution?"
               "How large are Kafka messages?"
```

### Custom Business Metrics

```java
@Component
public class PaymentObservability {

    private final MeterRegistry registry;

    // â”€â”€ Counters: what happened â”€â”€
    public void recordPaymentOutcome(PaymentType type, String outcome, String gateway) {
        registry.counter("payment.outcome",
            "type", type.name(),
            "outcome", outcome,         // "success", "failed", "rejected", "timeout"
            "gateway", gateway
        ).increment();
    }

    public void recordRetryAttempt(String service, int attempt, boolean succeeded) {
        registry.counter("resilience.retry",
            "service", service,
            "attempt", String.valueOf(attempt),
            "succeeded", String.valueOf(succeeded)
        ).increment();
    }

    public void recordDeadLetter(String topic, String errorType) {
        registry.counter("kafka.dead_letter",
            "original_topic", topic,
            "error_type", errorType
        ).increment();
    }

    // â”€â”€ Timers: how long things take â”€â”€
    public <T> T timeOperation(String name, String service, Supplier<T> operation) {
        return registry.timer("operation.duration",
            "name", name,
            "service", service
        ).record(operation);
    }

    // Convenient timer for gateway calls with outcome tagging
    public <T> T timeGatewayCall(String gateway, Supplier<T> call) {
        var sample = Timer.start(registry);
        String outcome = "success";
        try {
            return call.get();
        } catch (Exception e) {
            outcome = classifyError(e);
            throw e;
        } finally {
            sample.stop(registry.timer("gateway.call.duration",
                "gateway", gateway,
                "outcome", outcome));
        }
    }

    // â”€â”€ Gauges: current state â”€â”€
    @PostConstruct
    public void registerGauges() {
        // Outbox backlog
        Gauge.builder("outbox.pending.count", outboxRepository,
                repo -> repo.countByStatus(OutboxStatus.PENDING))
            .description("Number of outbox events awaiting publish")
            .register(registry);

        // Circuit breaker states (0=closed, 1=half-open, 2=open)
        circuitBreakerRegistry.getAllCircuitBreakers().forEach(cb ->
            Gauge.builder("circuit_breaker.state", cb,
                    b -> switch (b.getState()) {
                        case CLOSED -> 0; case HALF_OPEN -> 1; case OPEN -> 2;
                        default -> -1;
                    })
                .tag("name", cb.getName())
                .register(registry));

        // Active virtual threads (approximate)
        Gauge.builder("jvm.threads.virtual.active",
                () -> Thread.getAllStackTraces().keySet().stream()
                    .filter(Thread::isVirtual).count())
            .register(registry);
    }

    // â”€â”€ Distribution summaries: value distributions â”€â”€
    public void recordPaymentAmount(BigDecimal amount, String currency) {
        registry.summary("payment.amount",
            "currency", currency
        ).record(amount.doubleValue());
    }

    public void recordKafkaMessageSize(String topic, int sizeBytes) {
        registry.summary("kafka.message.size",
            "topic", topic
        ).record(sizeBytes);
    }

    // â”€â”€ SLI recording â”€â”€
    public void recordSli(String sliName, boolean good) {
        registry.counter("sli.events",
            "sli", sliName,
            "quality", good ? "good" : "bad"
        ).increment();
    }

    private String classifyError(Exception e) {
        if (e instanceof TimeoutException) return "timeout";
        if (e instanceof ConnectException) return "connection_refused";
        if (e instanceof HttpServerErrorException) return "server_error";
        if (e instanceof HttpClientErrorException) return "client_error";
        return "unknown";
    }
}
```

### Metric Naming Conventions

```
Format: <domain>.<entity>.<action>[.<detail>]

âœ… Good names:
  payment.processed.total           (counter)
  payment.gateway.duration          (timer)
  payment.amount                    (distribution)
  kafka.consumer.lag                (gauge)
  kafka.dead_letter.total           (counter)
  outbox.pending.count              (gauge)
  circuit_breaker.state             (gauge)
  http.server.requests              (timer â€” auto by Spring)

âŒ Bad names:
  paymentCount                      (no dots, no unit context)
  process_time                      (vague â€” what process?)
  errors                            (what kind? where?)
  data                              (meaningless)

Tag rules:
  â€¢ Low cardinality ONLY (< 100 unique values per tag)
  â€¢ âŒ NEVER tag with: userId, paymentId, IP address, email
  â€¢ âœ… Tag with: status, type, service, gateway, outcome, region
```

### Histogram Bucket Configuration

```yaml
management:
  metrics:
    distribution:
      # Enable percentile histograms for specific metrics
      percentiles-histogram:
        http.server.requests: true
        gateway.call.duration: true
        payment.gateway.duration: true
      # Define percentiles to publish
      percentiles:
        http.server.requests: 0.5, 0.9, 0.95, 0.99
        gateway.call.duration: 0.5, 0.95, 0.99
      # SLA boundaries (for counting requests within latency bands)
      slo:
        http.server.requests: 50ms, 100ms, 250ms, 500ms, 1s, 5s
      # Min/max expected values (optimizes bucket distribution)
      minimum-expected-value:
        http.server.requests: 1ms
        gateway.call.duration: 10ms
      maximum-expected-value:
        http.server.requests: 10s
        gateway.call.duration: 30s
```

---

## ğŸ“ Pattern 2: Structured Logging

### Log Levels â€” When to Use Each

```
TRACE   Detailed debugging â€” variable values, loop iterations
        NEVER in production. Enabled temporarily for specific loggers.

DEBUG   Diagnostic info â€” method entry/exit, decision branches
        Disabled in prod by default. Enable per-logger for investigation.

INFO    Business events â€” payment processed, order created, service started
        The "story" of what the system is doing. Primary production level.

WARN    Unexpected but handled â€” retry triggered, fallback used, slow query
        System continues normally but something is suboptimal.

ERROR   Failure requiring attention â€” unhandled exception, DLT message,
        compensation failure. Should trigger an alert if sustained.

FATAL   System cannot continue â€” missing config, corrupt state
        (Rarely used in Spring Boot â€” usually throws and exits)
```

### Structured Logging Configuration

```xml
<!-- logback-spring.xml -->
<configuration>

    <!-- â”€â”€ Dev profile: human-readable â”€â”€ -->
    <springProfile name="local">
        <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <pattern>%d{HH:mm:ss.SSS} %highlight(%-5level) [%thread] %cyan(%logger{36}) - %msg%n</pattern>
            </encoder>
        </appender>
        <root level="INFO">
            <appender-ref ref="CONSOLE" />
        </root>
    </springProfile>

    <!-- â”€â”€ Production: JSON for log aggregation â”€â”€ -->
    <springProfile name="prod,staging">
        <appender name="JSON" class="ch.qos.logback.core.ConsoleAppender">
            <encoder class="net.logstash.logback.encoder.LogstashEncoder">
                <includeMdcKeyName>traceId</includeMdcKeyName>
                <includeMdcKeyName>spanId</includeMdcKeyName>
                <includeMdcKeyName>correlationId</includeMdcKeyName>
                <includeMdcKeyName>paymentId</includeMdcKeyName>
                <includeMdcKeyName>customerId</includeMdcKeyName>
                <customFields>
                    {"service":"${SERVICE_NAME:-payment-service}",
                     "environment":"${ENVIRONMENT:-unknown}",
                     "instance":"${HOSTNAME:-unknown}"}
                </customFields>
                <throwableConverter class="net.logstash.logback.stacktrace.ShortenedThrowableConverter">
                    <maxDepthPerThrowable>30</maxDepthPerThrowable>
                    <shortenedClassNameLength>36</shortenedClassNameLength>
                </throwableConverter>
            </encoder>
        </appender>
        <root level="WARN">
            <appender-ref ref="JSON" />
        </root>
        <logger name="com.example.payment" level="INFO" />
    </springProfile>

</configuration>
```

### MDC Context Propagation

```java
// â”€â”€ Request context filter (HTTP) â”€â”€
@Component
@Order(Ordered.HIGHEST_PRECEDENCE)
public class ObservabilityFilter extends OncePerRequestFilter {

    @Override
    protected void doFilterInternal(HttpServletRequest request,
            HttpServletResponse response, FilterChain chain) throws Exception {

        String traceId = Optional.ofNullable(request.getHeader("X-Trace-Id"))
            .orElse(UUID.randomUUID().toString());
        String correlationId = Optional.ofNullable(request.getHeader("X-Correlation-Id"))
            .orElse(traceId);

        MDC.put("traceId", traceId);
        MDC.put("correlationId", correlationId);
        MDC.put("method", request.getMethod());
        MDC.put("path", request.getRequestURI());
        MDC.put("clientIp", request.getRemoteAddr());

        response.setHeader("X-Trace-Id", traceId);

        try {
            chain.doFilter(request, response);
        } finally {
            // Log request completion
            log.info("Request completed: {} {} â†’ {} in {}ms",
                request.getMethod(), request.getRequestURI(),
                response.getStatus(), System.currentTimeMillis() - getStartTime(request));
            MDC.clear();
        }
    }
}

// â”€â”€ Kafka consumer context â”€â”€
@Component
public class KafkaObservabilityInterceptor implements RecordInterceptor<String, Object> {

    @Override
    public ConsumerRecord<String, Object> intercept(ConsumerRecord<String, Object> record,
            Consumer<String, Object> consumer) {
        MDC.put("kafkaTopic", record.topic());
        MDC.put("kafkaPartition", String.valueOf(record.partition()));
        MDC.put("kafkaOffset", String.valueOf(record.offset()));

        extractHeader(record, "correlationId").ifPresent(id -> MDC.put("correlationId", id));
        extractHeader(record, "traceId").ifPresent(id -> MDC.put("traceId", id));
        extractHeader(record, "eventId").ifPresent(id -> MDC.put("eventId", id));

        return record;
    }

    @Override
    public void afterRecord(ConsumerRecord<String, Object> record,
            Consumer<String, Object> consumer) {
        MDC.clear();
    }
}

// â”€â”€ MDC propagation to virtual threads / CompletableFuture â”€â”€
@Component
public class MdcTaskDecorator implements TaskDecorator {

    @Override
    public Runnable decorate(Runnable runnable) {
        Map<String, String> context = MDC.getCopyOfContextMap();
        return () -> {
            if (context != null) MDC.setContextMap(context);
            try {
                runnable.run();
            } finally {
                MDC.clear();
            }
        };
    }
}
```

### What to Log â€” Decision Guide

```
âœ… ALWAYS LOG (INFO):
  â€¢ Service startup/shutdown with config summary
  â€¢ Business events (payment processed, order created)
  â€¢ External call entry/exit with timing
  â€¢ State transitions (payment INITIATED â†’ CAPTURED)
  â€¢ Authentication events (login, token refresh)

âœ… ALWAYS LOG (WARN):
  â€¢ Retry attempts with count and reason
  â€¢ Circuit breaker state changes
  â€¢ Fallback activations
  â€¢ Slow queries (> threshold)
  â€¢ Approaching resource limits (pool 80% full)

âœ… ALWAYS LOG (ERROR):
  â€¢ Unhandled exceptions with full stack trace
  â€¢ Dead-letter messages with payload context
  â€¢ Compensation failures
  â€¢ Data inconsistencies detected
  â€¢ External service permanent failures

âŒ NEVER LOG:
  â€¢ Passwords, tokens, API keys, secrets
  â€¢ Full credit card numbers (PCI)
  â€¢ PII in plain text (use masking)
  â€¢ Every SQL/MongoDB query (use DEBUG level)
  â€¢ Success of routine health checks
  â€¢ Request/response bodies in production (use DEBUG)
```

### PII Masking

```java
@Component
public class PiiMasker {

    public static String maskEmail(String email) {
        if (email == null) return null;
        int atIdx = email.indexOf('@');
        if (atIdx <= 1) return "***@" + email.substring(atIdx + 1);
        return email.charAt(0) + "***" + email.substring(atIdx);
    }

    public static String maskCardNumber(String number) {
        if (number == null || number.length() < 4) return "****";
        return "****-****-****-" + number.substring(number.length() - 4);
    }

    public static String maskAccountId(String id) {
        if (id == null || id.length() < 4) return "****";
        return "***" + id.substring(id.length() - 4);
    }
}

// Usage in logs
log.info("Processing payment for customer={}, card={}",
    PiiMasker.maskEmail(customer.email()),
    PiiMasker.maskCardNumber(card.number()));
// Output: "Processing payment for customer=j***@email.com, card=****-****-****-4242"
```

---

## ğŸ”— Pattern 3: Distributed Tracing

### OpenTelemetry Integration (Spring Boot 3.x)

```yaml
# application.yml â€” Spring Boot 3.x with Micrometer Tracing
management:
  tracing:
    sampling:
      probability: 1.0       # 100% in dev/staging, 0.1 (10%) in prod
    propagation:
      type: w3c              # W3C Trace Context standard
  otlp:
    tracing:
      endpoint: ${OTEL_EXPORTER_ENDPOINT:http://localhost:4318/v1/traces}
```

```xml
<!-- pom.xml dependencies -->
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-tracing-bridge-otel</artifactId>
</dependency>
<dependency>
    <groupId>io.opentelemetry</groupId>
    <artifactId>opentelemetry-exporter-otlp</artifactId>
</dependency>
```

### Custom Spans for Business Logic

```java
@Component
public class PaymentService {

    private final ObservationRegistry observationRegistry;

    public PaymentResult processPayment(PaymentRequest request) {
        return Observation.createNotStarted("payment.process", observationRegistry)
            .lowCardinalityKeyValue("payment.type", request.type().name())
            .lowCardinalityKeyValue("payment.currency", request.currency())
            .highCardinalityKeyValue("payment.id", request.paymentId())    // For traces only
            .observe(() -> {
                validate(request);
                var fraudResult = checkFraud(request);
                var chargeResult = chargeGateway(request);
                return toResult(chargeResult);
            });
    }

    private FraudResult checkFraud(PaymentRequest request) {
        return Observation.createNotStarted("payment.fraud_check", observationRegistry)
            .lowCardinalityKeyValue("fraud.provider", "internal")
            .observe(() -> fraudService.evaluate(request));
    }

    private ChargeResult chargeGateway(PaymentRequest request) {
        return Observation.createNotStarted("payment.gateway_charge", observationRegistry)
            .lowCardinalityKeyValue("gateway", "stripe")
            .observe(() -> gateway.charge(request));
    }
}
```

### Trace Context Propagation Across Kafka

```java
// â”€â”€ Producer: inject trace context into Kafka headers â”€â”€
@Component
public class TracingProducerInterceptor implements ProducerInterceptor<String, Object> {

    @Override
    public ProducerRecord<String, Object> onSend(ProducerRecord<String, Object> record) {
        // Micrometer Tracing auto-injects W3C traceparent header
        // when using ObservationRegistry-aware KafkaTemplate
        return record;
    }
}

// â”€â”€ Spring Boot 3.x auto-propagation â”€â”€
// Just enable:
spring.kafka.producer.properties.interceptor.classes=\
  io.opentelemetry.instrumentation.kafkaclients.TracingProducerInterceptor
spring.kafka.consumer.properties.interceptor.classes=\
  io.opentelemetry.instrumentation.kafkaclients.TracingConsumerInterceptor
```

### Trace Visualization â€” What to See

```
Payment Request (trace: abc-123)
â”‚
â”œâ”€â”€ payment-service: POST /api/v1/payments (250ms total)
â”‚   â”œâ”€â”€ payment.validate (5ms)
â”‚   â”œâ”€â”€ payment.fraud_check (80ms)
â”‚   â”‚   â””â”€â”€ fraud-service: POST /api/v1/evaluate (75ms)
â”‚   â”‚       â””â”€â”€ mongodb: find (3ms)
â”‚   â”œâ”€â”€ payment.gateway_charge (150ms)
â”‚   â”‚   â””â”€â”€ stripe-api: POST /charges (145ms)   â† Bottleneck!
â”‚   â””â”€â”€ mongodb: insertOne (8ms)
â”‚
â”œâ”€â”€ kafka: payments.transaction.captured (async)
â”‚   â””â”€â”€ settlement-service: consume (45ms)
â”‚       â”œâ”€â”€ mongodb: findAndModify (5ms)
â”‚       â””â”€â”€ notification-service: POST /notify (30ms)
â”‚
â””â”€â”€ kafka: payments.audit.log (async)
    â””â”€â”€ audit-service: consume (10ms)
        â””â”€â”€ elasticsearch: index (8ms)
```

---

## ğŸš¨ Pattern 4: Alerting Strategy

### Alert Severity Levels

```
PAGE (Critical) â€” Wake someone up at 3am
  â€¢ Service completely down
  â€¢ Error rate > 5% for > 2 minutes
  â€¢ SLO burn rate critical
  â€¢ Data loss detected (DLT overflow)
  â€¢ Payment success rate dropped

TICKET (Warning) â€” Address during business hours
  â€¢ Elevated latency (p99 > 2x normal)
  â€¢ Consumer lag growing steadily
  â€¢ Retry rate > 20%
  â€¢ Circuit breaker flapping
  â€¢ Disk/memory approaching limit

INFO â€” Dashboard only, no notification
  â€¢ Deployment completed
  â€¢ Routine circuit breaker trip/recovery
  â€¢ Scheduled job completion
  â€¢ Traffic pattern changes
```

### Alert Rules â€” Production Templates

```yaml
groups:
  # â”€â”€ Availability â”€â”€
  - name: availability
    rules:
      # Service down
      - alert: ServiceDown
        expr: up{job="payment-service"} == 0
        for: 1m
        labels:
          severity: page
        annotations:
          summary: "Payment service instance {{ $labels.instance }} is DOWN"

      # High error rate
      - alert: HighErrorRate
        expr: >
          sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m]))
          /
          sum(rate(http_server_requests_seconds_count[5m]))
          > 0.05
        for: 2m
        labels:
          severity: page
        annotations:
          summary: "Error rate is {{ $value | humanizePercentage }} (> 5%)"

  # â”€â”€ Latency â”€â”€
  - name: latency
    rules:
      # p99 latency spike
      - alert: HighP99Latency
        expr: >
          histogram_quantile(0.99,
            sum(rate(http_server_requests_seconds_bucket[5m])) by (le))
          > 2
        for: 5m
        labels:
          severity: ticket
        annotations:
          summary: "p99 latency is {{ $value | humanizeDuration }} (> 2s)"

      # Gateway latency
      - alert: GatewayLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(gateway_call_duration_seconds_bucket[5m])) by (le, gateway))
          > 5
        for: 3m
        labels:
          severity: ticket
        annotations:
          summary: "Gateway {{ $labels.gateway }} p95 latency {{ $value | humanizeDuration }}"

  # â”€â”€ Kafka â”€â”€
  - name: kafka
    rules:
      # Consumer lag
      - alert: KafkaConsumerLag
        expr: kafka_consumer_group_lag > 10000
        for: 10m
        labels:
          severity: ticket
        annotations:
          summary: "Consumer group {{ $labels.group }} lag: {{ $value }} on {{ $labels.topic }}"

      # Dead letters accumulating
      - alert: DeadLetterAccumulating
        expr: increase(kafka_dead_letter_total[1h]) > 50
        for: 0m
        labels:
          severity: page
        annotations:
          summary: "{{ $value }} dead letters in last hour on {{ $labels.original_topic }}"

  # â”€â”€ Resources â”€â”€
  - name: resources
    rules:
      # Heap approaching limit
      - alert: HighHeapUsage
        expr: jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"} > 0.85
        for: 5m
        labels:
          severity: ticket

      # Connection pool exhaustion
      - alert: ConnectionPoolExhausted
        expr: mongodb_driver_pool_waitqueuesize > 0
        for: 30s
        labels:
          severity: ticket

  # â”€â”€ Circuit Breakers â”€â”€
  - name: resilience
    rules:
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state == 2
        for: 0m
        labels:
          severity: page
        annotations:
          summary: "Circuit breaker {{ $labels.name }} is OPEN"

      - alert: HighRetryRate
        expr: >
          sum(rate(resilience_retry_total{succeeded="false"}[5m])) by (service)
          /
          sum(rate(resilience_retry_total[5m])) by (service)
          > 0.3
        for: 5m
        labels:
          severity: ticket
```

### Alert Anti-Patterns

```
âŒ Alerting on causes instead of symptoms
   "CPU > 80%" â†’ what's the user impact?
   âœ… Better: "p99 latency > 2s" or "error rate > 5%"

âŒ Too many non-actionable alerts â†’ alert fatigue
   âœ… Every alert must have a documented runbook action

âŒ No severity levels â†’ everything pages at 3am
   âœ… page = customer impact, ticket = degrade gracefully

âŒ Threshold too tight â†’ flapping alerts
   âœ… Use 'for' duration to avoid transient spikes

âŒ Missing alerts â†’ find out from customers
   âœ… Alert on SLO burn rate â€” catches issues before customers notice
```

---

## ğŸ¯ Pattern 5: SLI/SLO Definitions

### SLI Taxonomy for Payment Services

```
AVAILABILITY SLI:
  Definition: Proportion of successful requests
  Formula:    successful requests / total requests
  Good event: HTTP 2xx or 4xx (client error is "available" from server perspective)
  Bad event:  HTTP 5xx or timeout

LATENCY SLI:
  Definition: Proportion of requests faster than threshold
  Formula:    requests < 500ms / total requests
  Good event: Response time < 500ms
  Bad event:  Response time >= 500ms

CORRECTNESS SLI:
  Definition: Proportion of payments processed correctly
  Formula:    (payments - reconciliation mismatches) / payments
  Good event: Payment matches reconciliation
  Bad event:  Reconciliation mismatch

FRESHNESS SLI (for async):
  Definition: Proportion of events processed within SLA
  Formula:    events processed < 5min / total events
  Good event: Kafka consumer lag < 5 minutes
  Bad event:  Kafka consumer lag >= 5 minutes
```

### SLO Definitions

```yaml
# SLO Document â€” Payment Service
slos:
  availability:
    sli: "Successful HTTP responses (non-5xx) / total responses"
    target: 99.95%                    # ~22 min downtime/month
    window: 30 days (rolling)
    error_budget: 0.05%               # ~22 min/month
    burn_rate_alert:
      fast: 14.4x over 1h            # Consuming budget in ~2 days
      slow: 6x over 6h               # Consuming budget in ~5 days

  latency:
    sli: "Responses < 500ms / total responses"
    target: 99.0%                     # 1% of requests can be slow
    window: 30 days (rolling)
    tiers:
      p50: < 100ms
      p95: < 300ms
      p99: < 500ms

  payment_success:
    sli: "Successful payment captures / total payment attempts"
    target: 99.9%
    window: 7 days (rolling)
    exclusions:
      - "Legitimate fraud rejections"
      - "Invalid payment methods (client error)"

  event_freshness:
    sli: "Events processed within 5 minutes / total events"
    target: 99.5%
    window: 30 days (rolling)
```

### SLO Burn Rate Alerts (Multi-Window)

```yaml
# Fast burn â€” consuming error budget quickly (page immediately)
- alert: SLOFastBurn_Availability
  expr: >
    (
      sum(rate(http_server_requests_seconds_count{status=~"5.."}[1h]))
      /
      sum(rate(http_server_requests_seconds_count[1h]))
    ) > (14.4 * 0.0005)
    AND
    (
      sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m]))
      /
      sum(rate(http_server_requests_seconds_count[5m]))
    ) > (14.4 * 0.0005)
  for: 2m
  labels:
    severity: page
  annotations:
    summary: "Availability SLO fast burn: {{ $value | humanizePercentage }} error rate"

# Slow burn â€” steady error budget consumption (ticket)
- alert: SLOSlowBurn_Availability
  expr: >
    (
      sum(rate(http_server_requests_seconds_count{status=~"5.."}[6h]))
      /
      sum(rate(http_server_requests_seconds_count[6h]))
    ) > (6 * 0.0005)
    AND
    (
      sum(rate(http_server_requests_seconds_count{status=~"5.."}[30m]))
      /
      sum(rate(http_server_requests_seconds_count[30m]))
    ) > (6 * 0.0005)
  for: 15m
  labels:
    severity: ticket
  annotations:
    summary: "Availability SLO slow burn â€” error budget being consumed steadily"
```

---

## ğŸ“Š Pattern 6: Dashboards â€” What to Show

### Service Health Dashboard (The "Golden Signals")

```
Row 1: THE BIG NUMBERS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request Rate â”‚  Error Rate  â”‚ p50 Latency  â”‚ p99 Latency  â”‚
â”‚   1,234/sec  â”‚    0.12%     â”‚    45ms      â”‚   320ms      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Row 2: SLO STATUS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Availability: 99.97% (budget: 68% remaining)             â”‚
â”‚ Latency:      99.3%  (budget: 30% remaining) âš ï¸          â”‚
â”‚ Payment Success: 99.95% (budget: 50% remaining)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Row 3: TRAFFIC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Request rate over time - line chart]                     â”‚
â”‚ [Error rate over time - line chart with threshold line]   â”‚
â”‚ [Latency percentiles over time - p50, p95, p99]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Row 4: DEPENDENCIES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Payment Gateway: âœ… CLOSED (0.5% failure)                â”‚
â”‚ Fraud Service:   âœ… CLOSED (0.1% failure)                â”‚
â”‚ Account Service: âš ï¸ HALF-OPEN (45% failure)              â”‚
â”‚ MongoDB:         âœ… Pool 23/50 (p99: 8ms)                â”‚
â”‚ Kafka:           âœ… Lag: 45 msgs (consumer: 3/3 running) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Row 5: INFRASTRUCTURE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Heap usage over time]  [GC pause histogram]             â”‚
â”‚ [CPU usage]             [Thread count]                    â”‚
â”‚ [Connection pools]      [Kafka consumer lag]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Grafana Queries (PromQL)

```promql
# Request rate
sum(rate(http_server_requests_seconds_count[5m]))

# Error rate
sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m]))
/ sum(rate(http_server_requests_seconds_count[5m]))

# p99 latency
histogram_quantile(0.99, sum(rate(http_server_requests_seconds_bucket[5m])) by (le))

# Availability (over 30 days)
1 - (
  sum(increase(http_server_requests_seconds_count{status=~"5.."}[30d]))
  / sum(increase(http_server_requests_seconds_count[30d]))
)

# Error budget remaining
1 - (
  sum(increase(http_server_requests_seconds_count{status=~"5.."}[30d]))
  / sum(increase(http_server_requests_seconds_count[30d]))
  / (1 - 0.9995)  # 99.95% SLO target
)

# Gateway success rate per gateway
sum(rate(gateway_call_duration_seconds_count{outcome="success"}[5m])) by (gateway)
/ sum(rate(gateway_call_duration_seconds_count[5m])) by (gateway)

# Kafka end-to-end latency (event time to processing time)
histogram_quantile(0.99, sum(rate(kafka_consumer_processing_lag_seconds_bucket[5m])) by (le, topic))
```

---

## ğŸš« Observability Anti-Patterns

| Anti-Pattern | Why It's Dangerous | Fix |
|---|---|---|
| **High-cardinality metric tags** | Prometheus memory explosion (userId, orderId) | Tags < 100 unique values |
| **Logging request/response bodies in prod** | Performance hit, PII exposure, disk fill | DEBUG level only, mask PII |
| **No correlation ID** | Can't trace a request across services | Propagate correlationId in every hop |
| **Metrics without alerts** | Dashboard exists but nobody watches it | Every metric that matters gets an alert |
| **Alerts without runbooks** | Alert fires, engineer doesn't know what to do | Document action for every alert |
| **Sampling traces at 100% in prod** | Storage cost, performance overhead | 10% sampling + always sample errors |
| **Alerting on causes, not symptoms** | "CPU high" doesn't mean users are affected | Alert on latency, errors, throughput |
| **No MDC cleanup** | Context leaks between requests on thread reuse | Always MDC.clear() in finally block |
| **Logging secrets** | Security breach in log aggregation | Automated secret scanning, PII masking |
| **Missing async context propagation** | Traces break at Kafka/async boundaries | Propagate trace context in headers |

---

## ğŸ’¡ Golden Rules of Observability

```
1.  METRICS tell you WHAT is wrong. LOGS tell you WHY. TRACES tell you WHERE.
2.  Alert on SYMPTOMS (latency, errors) not CAUSES (CPU, memory).
3.  Every alert must have a RUNBOOK â€” if there's no action, it's not an alert.
4.  CORRELATION ID in every log line, every event, every trace â€” non-negotiable.
5.  SLOs define what "good" means â€” without them, you're guessing.
6.  Error budget is your friend â€” it tells you when to push features vs fix reliability.
7.  LOW CARDINALITY tags only â€” userId in a metric tag will bankrupt your monitoring.
8.  STRUCTURED JSON logging in production â€” grep is not an observability strategy.
9.  SAMPLE traces in production (10%) but ALWAYS capture errors and slow requests.
10. Observability is a PRODUCT FEATURE â€” invest in it before the first outage, not after.
```

---

*Last updated: February 2026 | Stack: Java 21+ / Spring Boot 3.x / Micrometer / Prometheus / Grafana / OpenTelemetry*
